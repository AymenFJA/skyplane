{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skylark.utils import logger\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(globals()[\"_dh\"][0]).parent / \"data\"\n",
    "figure_dir = data_dir / \"figures\" / \"get_transfer_costs\"\n",
    "figure_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "plt.style.use(\"seaborn-bright\")\n",
    "plt.set_cmap(\"plasma\")\n",
    "\n",
    "data_link = \"https://b0.p.awsstatic.com/pricing/2.0/meteredUnitMaps/datatransfer/USD/current/datatransfer.json\"\n",
    "data = requests.get(data_link).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"regions\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get region names\n",
    "region_name_map = {\n",
    "    \"Africa (Cape Town)\": \"af-south-1\",\n",
    "    \"Asia Pacific (Hong Kong)\": \"ap-east-1\",\n",
    "    \"Asia Pacific (Mumbai)\": \"ap-south-1\",\n",
    "    \"Asia Pacific (Osaka)\": \"ap-northeast-3\",\n",
    "    \"Asia Pacific (Seoul)\": \"ap-northeast-2\",\n",
    "    \"Asia Pacific (Singapore)\": \"ap-southeast-1\",\n",
    "    \"Asia Pacific (Sydney)\": \"ap-southeast-2\",\n",
    "    \"Asia Pacific (Jakarta)\": \"ap-southeast-3\",\n",
    "    \"Asia Pacific (Tokyo)\": \"ap-northeast-1\",\n",
    "    \"AWS GovCloud (US-East)\": \"us-gov-east-1\",\n",
    "    \"AWS GovCloud (US-West)\": \"us-gov-west-1\",\n",
    "    \"Canada (Central)\": \"ca-central-1\",\n",
    "    \"Europe (Frankfurt)\": \"eu-central-1\",\n",
    "    \"Europe (Ireland)\": \"eu-west-1\",\n",
    "    \"Europe (London)\": \"eu-west-2\",\n",
    "    \"Europe (Milan)\": \"eu-south-1\",\n",
    "    \"Europe (Paris)\": \"eu-west-3\",\n",
    "    \"Europe (Stockholm)\": \"eu-north-1\",\n",
    "    \"Middle East (Bahrain)\": \"me-south-1\",\n",
    "    \"South America (São Paulo)\": \"sa-east-1\",\n",
    "    \"US East (N. Virginia)\": \"us-east-1\",\n",
    "    \"US East (Ohio)\": \"us-east-2\",\n",
    "    \"US West (N. California)\": \"us-west-1\",\n",
    "    \"US West (Oregon)\": \"us-west-2\",\n",
    "}\n",
    "new_region_name_map_items = {}\n",
    "for region_english, region_code in region_name_map.items():\n",
    "    if region_english.startswith(\"Europe\"):\n",
    "        new_region_name_map_items[region_english.replace(\"Europe\", \"EU\")] = region_code\n",
    "        new_name = region_english.replace(\"Europe\", \"EU\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        new_region_name_map_items[new_name] = region_code\n",
    "    if \"(\" in region_english:\n",
    "        new_name = region_english.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        new_region_name_map_items[new_name] = region_code\n",
    "region_name_map.update(new_region_name_map_items)\n",
    "region_name_map[\"South America Sao Paulo\"] = \"sa-east-1\"\n",
    "region_name_map[\"South America (Sao Paulo)\"] = \"sa-east-1\"\n",
    "region_name_map[\"South America (São Paulo)\"] = \"sa-east-1\"\n",
    "region_name_map[\"South America São Paulo\"] = \"sa-east-1\"\n",
    "region_name_map[\"US East N. Virginia\"] = \"us-east-1\"\n",
    "region_name_map[\"US East N Virginia\"] = \"us-east-1\"\n",
    "region_name_map[\"US West N. California\"] = \"us-west-1\"\n",
    "region_name_map[\"US West N California\"] = \"us-west-1\"\n",
    "\n",
    "# parse json\n",
    "cost_per_gb = []\n",
    "unparsed_regions = []\n",
    "for src_region, region_data in data[\"regions\"].items():\n",
    "    src = region_name_map.get(src_region)\n",
    "    if src is None:\n",
    "        logger.info(f\"Missing region {src} ({src_region})\")\n",
    "        unparsed_regions.append(src_region)\n",
    "    else:\n",
    "        for dst_region, dst_region_data in region_data.items():\n",
    "            cost = float(dst_region_data[\"price\"])\n",
    "            if dst_region.startswith(\"DataTransfer External Inbound\"):\n",
    "                assert cost == 0.0\n",
    "            elif dst_region.startswith(\"DataTransfer External Outbound\"):\n",
    "                regex = re.compile(r\"DataTransfer External Outbound (?P<volume>.*)\")\n",
    "                match = regex.search(dst_region)\n",
    "                if match:\n",
    "                    if match.group(\"volume\") == \"Next 10 TB\":\n",
    "                        cost_per_gb.append(dict(src=src, dst=\"internet\", cost=cost))\n",
    "                else:\n",
    "                    logger.error(f\"Could not parse {dst_region}\")\n",
    "            elif dst_region.startswith(\"DataTransfer InterRegion Outbound to\"):\n",
    "                regex = re.compile(r\"DataTransfer InterRegion Outbound to (?P<dst_region>.*)\")\n",
    "                match = regex.search(dst_region)\n",
    "                if match:\n",
    "                    dst = region_name_map.get(match.group(\"dst_region\"))\n",
    "                    if dst is None:\n",
    "                        unparsed_regions.append(match.group(\"dst_region\"))\n",
    "                    else:\n",
    "                        cost_per_gb.append(dict(src=src, dst=dst, cost=cost))\n",
    "                else:\n",
    "                    logger.error(f\"Could not parse {dst_region}\")\n",
    "            elif (\n",
    "                dst_region.startswith(\"Cloudfrontless\")\n",
    "                or dst_region.startswith(\"DirectoryService\")\n",
    "                or dst_region.startswith(\"Backup\")\n",
    "                or dst_region.startswith(\"RDS\")\n",
    "                or dst_region.startswith(\"FSX\")\n",
    "            ):\n",
    "                pass\n",
    "            else:\n",
    "                unparsed_regions.append(dst_region)\n",
    "df = pd.DataFrame(cost_per_gb)\n",
    "df.to_csv(data_dir / \"..\" / \"profiles\" / \"aws_transfer_costs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gcp_regions = pd.read_csv(data_dir / \"..\" / \"profiles\" / \"gcp_regions.csv\")\n",
    "gcp_region_map = dict(zip(df_gcp_regions[\"GCP name\"], df_gcp_regions[\"GCP code\"]))\n",
    "\n",
    "df_gcp = pd.read_csv(data_dir / \"..\" / \"profiles\" / \"gcp_raw_pricing_api.csv\")\n",
    "df_gcp = df_gcp[df_gcp[\"Service description\"] == \"Compute Engine\"]\n",
    "df_gcp = df_gcp[df_gcp[\"Product taxonomy\"].str.startswith(\"GCP > Network > Egress\")]\n",
    "df_gcp = df_gcp[df_gcp[\"Product taxonomy\"] != \"GCP > Network > Egress > GCE > Premium > PD\"]\n",
    "df_gcp[\"Egress tier\"] = df_gcp[\"Product taxonomy\"].str.split(\">\").str[-1]\n",
    "df_gcp[[\"SKU description\", \"Egress tier\", \"List price ($)\", \"Tiered usage start\", \"Unit description\", \"Per unit quantity\"]]\n",
    "\n",
    "out_rows = []\n",
    "bw_tuples = []\n",
    "mismatches = []\n",
    "for row in df_gcp.iterrows():\n",
    "    row = row[1]\n",
    "    if \"Internet\" in row[\"SKU description\"] and \"from\" in row[\"SKU description\"] and \"to\" in row[\"SKU description\"]:\n",
    "        regex = re.compile(r\"Network Internet(?P<tier>.*) Egress from (?P<region>.*) to (?P<dst_region>.*)\")\n",
    "        match = regex.search(row[\"SKU description\"])\n",
    "        if match:\n",
    "            src = match.group(\"region\")\n",
    "            dst = match.group(\"dst_region\")\n",
    "            bw_tuples.append(\n",
    "                dict(\n",
    "                    src=src,\n",
    "                    dst=dst,\n",
    "                    cost=row[\"List price ($)\"],\n",
    "                    unit=row[\"Unit description\"],\n",
    "                    quantity=row[\"Per unit quantity\"],\n",
    "                    tier=row[\"Egress tier\"],\n",
    "                    tier_start=row[\"Tiered usage start\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            logger.error(f'Could not parse {row[\"SKU description\"]}')\n",
    "    elif \"Internet\" in row[\"SKU description\"] and \"from\" in row[\"SKU description\"] and \"to\" not in row[\"SKU description\"]:\n",
    "        regex = re.compile(r\"Network Internet(?P<tier>.*) Egress from (?P<region>.*)\")\n",
    "        match = regex.search(row[\"SKU description\"])\n",
    "        if match:\n",
    "            src = match.group(\"region\")\n",
    "            dst = \"internet\"\n",
    "            bw_tuples.append(\n",
    "                dict(\n",
    "                    src=src,\n",
    "                    dst=dst,\n",
    "                    cost=row[\"List price ($)\"],\n",
    "                    unit=row[\"Unit description\"],\n",
    "                    quantity=row[\"Per unit quantity\"],\n",
    "                    tier=row[\"Egress tier\"],\n",
    "                    tier_start=row[\"Tiered usage start\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            logger.error(f'Could not parse {row[\"SKU description\"]}')\n",
    "    elif \"from\" in row[\"SKU description\"] and \"to\" in row[\"SKU description\"]:\n",
    "        regex = re.compile(r\".*from (?P<from>.*) to (?P<to>.*)\")\n",
    "        match = regex.search(row[\"SKU description\"])\n",
    "        if match:\n",
    "            src = match.group(\"from\")\n",
    "            dst = match.group(\"to\")\n",
    "            bw_tuples.append(\n",
    "                dict(\n",
    "                    src=src,\n",
    "                    dst=dst,\n",
    "                    cost=row[\"List price ($)\"],\n",
    "                    unit=row[\"Unit description\"],\n",
    "                    quantity=row[\"Per unit quantity\"],\n",
    "                    tier=row[\"Egress tier\"],\n",
    "                    tier_start=row[\"Tiered usage start\"],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            logger.error(f'Could not parse {row[\"SKU description\"]}')\n",
    "    else:\n",
    "        out_rows.append(row)\n",
    "df_gcp_bw = pd.DataFrame(bw_tuples)\n",
    "df_gcp_out = pd.DataFrame(out_rows)\n",
    "# df_gcp_bw.to_csv(data_dir / '..' / 'profiles' / 'gcp_bw_costs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match src, dst to region names\n",
    "unmatched_pairs = []\n",
    "out_rows = []\n",
    "print(gcp_region_map.keys())\n",
    "for row in df_gcp_bw.iterrows():\n",
    "    row = row[1]\n",
    "    src = row[\"src\"]\n",
    "    dst = row[\"dst\"]\n",
    "    if src in gcp_region_map:\n",
    "        row[\"src\"] = gcp_region_map[src]\n",
    "    else:\n",
    "        unmatched_pairs.append((src, dst))\n",
    "    if dst in gcp_region_map:\n",
    "        row[\"dst\"] = gcp_region_map[dst]\n",
    "    elif dst == \"internet\":\n",
    "        row[\"dst\"] = \"internet\"\n",
    "    else:\n",
    "        unmatched_pairs.append((src, dst))\n",
    "    out_rows.append(row)\n",
    "df_gcp_bw = pd.DataFrame(out_rows)\n",
    "df_gcp_bw = df_gcp_bw[df_gcp_bw[\"tier_start\"] == 0.0]\n",
    "df_gcp_bw.to_csv(data_dir / \"..\" / \"profiles\" / \"gcp_bw_costs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(unmatched_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "189b01ff19767b4e7acbd9e36bed775a02bd58e988a3c20d368ed4c0c3290bac"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
